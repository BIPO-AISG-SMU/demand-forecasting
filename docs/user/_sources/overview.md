# Project Overview

## Purpose of Documentation

This documentation serves as a guide for the technical user in deploying and customising the Demand Forecasting module. 

The reader is expected to have familiarity with: 
1. the Linux environment 
2. using the command line on Linux and Windows
3. managing Conda environments
4. Docker

This guide does not provide any information pertaining to deployment on any cloud service.

## Architecture

### Software Components

The following diagram illustrates the high-level components of the machine learning pipeline and how they interact with one another.

![Demand Forecasting Deployment Architecture](assets/ml-pipeline.png)

### Deployment Architecture
The following diagram illustrates how a user or an external system may interact with the Demand Forecasting module, as well as the high-level internal structure of each submodule.

![Demand Forecasting Deployment Architecture](assets/deployment-architecture.png)

## Deployment Package Contents
```
└── bipo_demand_forecasting/
    ├── conf/ (Created and to be bind mounted)
    |    └── base/
    |        ├── parameters
    |        |   ├── model_training.yml
    |        |   └── data_split.yml
    |        ├── parameters.yml
    |        ├── logging.yml
    |        ├── inference_parameters.yml
    |        ├── constants.yml
    |        └── catalog.yml
    ├── data/ (Created and to be bind mounted)
    ├── logs/ (Created and to be bind mounted)
    ├── models/ (Created and to be bind mounted)
    ├── mlruns/ (Created and to be bind mounted)
    ├── notebooks/
    |   └── Baseline_Model_LSTM_4bins.ipynb
    ├── docker/
    |   ├── bipo-model-serving.Dockerfile
    |   └── bipo-model-training-cpu.Dockerfile
    ├── docs/
    ├── scripts/
    |   ├── build_push_kedro_data_training_pipeline.bat
    |   ├── build_push_model_training.bat
    |   ├── kedro_docker_run_testing.bat
    |   ├── run_data_pipeline.bat
    |   └── run_model_training.bat
    ├── src/
    |   └── requirements.txt
    └── bipo-conda-env.yml
```

### Configuration

| Path | Description |
| :- | - |
| **conf/base/** | Contains all `.yml` configuration files. |
| `parameters.yml` | System-wide adjustable pipeline parameters. |
| `inference_parameters.yml` | Adjustable inference pipeline parameters. |
| `logging.yml` | Logging configuration for Kedro pipelines. |
| `constants.yml` | Constants defining default values for key pipeline parameters. |
| `catalog.yml` | Registry of all data sources available for use by the project. See [The Kedro Data Catalog](https://docs.kedro.org/en/stable/data/data_catalog.html) for a complete guide.|
| parameters/`model_training.yml` | Adjustable parameters specific to the `model_training` Kedro pipeline |
| parameters/`data_split.yml` | Adjustable parameters specific to the `data_split` Kedro pipeline |

The files above have been configured with the default recommended parameters to be used on the default deployment architecture. Developers experimenting with different configurations or testing the module on a different environment should create a subdirectory within `conf` for the specific purpose (e.g. `conf/local` or `conf/aws`).

### Data

| Path | Description |
| :- | - |
| **data/** | |
|01_raw | Contains raw files from various data sources. |
|02_dataloader | Contains intermediate output files processed by the `data_loader` Kedro node. |
|03_data_preprocessing | Contains intermediate output files processed by the `data_preprocessing` Kedro node. |
|04_data_split | Contains intermediate output files generated by the `data_split` Kedro node. |
|04a_time_agnostic_feature_engineering | Contains intermediate output files generated by the `time_agnostic_feature_engineering` Kedro node. |
|05_feature_engineering | Contains intermediate output files generated by the `feature_engineering` Kedro node. |
|06_model_specific_preprocessing | Contains intermediate output files processed by the `model_specific_preprocessing` Kedro node. |
|07_model_evaluation | Contains artefacts generated by the `model_evaluation` Kedro node. |
|08_model_inference_output| Subdirectory to store intermediate API request and response in JSON format.|

### Docker images

| Path | Description | Release Date |
| :- | - | - |
| **docker/** | Contains all Docker archive files. Containerised image are exported into `.tar` archive files and stored here for loading into AWS ECR.| |
| `100E_BIPO_docker_inference.tar` | Docker image archive containing model serving API and inference submodule. | 18/08/2023 (initial deployment) |
| `100E_BIPO_docker_training.tar` | Docker image archive containing model training submodule. | 19/10/2023 (final deployment) |
| `bipo-model-training-cpu.Dockerfile` | Dockerfile used to build the Docker image in `100E_BIPO_docker_training.tar` | 19/10/2023 (final deployment) |
| `bipo-model-serving.Dockerfile` | Dockerfile used to build the Docker image in `100E_BIPO_docker_inference.tar` | 19/10/2023 (final deployment) |

### Logs

| Path | Description |
| :- | - |
| **logs/** | Contains all logs generated by the system.|
|`info.log`|Logs events that occur in normal system flow.|
|`error.log`|Logs events where errors occur that interrupts the normal system flow.|

### Trained model file(s)

The trained model file would be stored in an AWS S3 bucket. To ensure successful download, necessary S3 permissions are required to be configured in AWS Roles settings by the AWS administrator. 

| Path | Description |
| :- | - |
| **models/** | Contains all trained models saved in pickle format used in the pipeline, with `.pkl` as extension. |
|`orderedmodel_20231019.pkl`|Model weights of [OrderedModel](https://www.statsmodels.org/dev/generated/statsmodels.miscmodels.ordinal_model.OrderedModel.html) from the [statsmodels](https://www.statsmodels.org) library|
|`ebm_20231019.pkl`|Model weights of [ExplainableBoostingClassifier](https://interpret.ml/docs/ExplainableBoostingClassifier.html) from the [InterpretML](https://interpret.ml/) library|

### Notebooks

| Path | Description |
| :- | - |
| **notebooks/** | Contains Jupyter notebooks with standalone code (not part of ML pipeline).|
|`Baseline_Model_LSTM_4bins.ipynb`|Notebooks used to predict 4 categories of sales using the LSTM (TSAI) baseline model.|

### Scripts

| Path | Description |
| :- | - |
| **scripts/** | Contains scripts to run on the host machines. |
|`apt-install.sh`|Script to install all required packages on an Ubuntu deployment environment.|
|`run_inference_endpoint.sh`|Script to load and run the model serving Docker container.|
|`run_model_training.bat`|Script to load and run the model training Docker container.|
|`run_data_pipeline.bat`|Script to load and run the data pipeline (within the training Docker container) to generate new datasets for model training.|
| `build_push_kedro_data_training_pipeline.bat`| Builds and pushes Kedro data pipeline Docker image to registry. |
| `build_push_model_training.bat`              | Creates and uploads model training Docker image to registry.  |
| `kedro_docker_run_testing.bat`               | Tests Kedro pipeline execution within a Docker container.     |
| `run_data_pipeline.bat`                      | Runs data preprocessing pipeline in Docker for dataset preparation. |

### OS port usage

Any firewall configurations or other applications installed should not be using the stated ports:

| Port | Description |
| :- | - |
| 2375 | Docker unencrypted communication |
| 2376 | Docker encrypted communication |
| 8000 | FastAPI endpoint|
