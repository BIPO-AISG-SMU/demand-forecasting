"""
Contains 2 main pipelines 
1. create_feature_engineering_pipeline
2. create_feature_selection_pipeline

One pipeline for each outlet. Individual pipelines are then added into a main Pipeline. 
"""
import os
import pandas as pd
from kedro.pipeline import Pipeline, node, pipeline
from kedro.io import DataCatalog
from .nodes import *
from bipo.utils import get_input_output_folder, get_project_path
from kedro.config import ConfigLoader
import logging

logging = logging.getLogger(__name__)
# Create an empty data catalog to store input and output filepaths
data_catalog = DataCatalog()

# Instantiate config
project_path = get_project_path()
conf_loader = ConfigLoader(conf_source=project_path / "conf")
conf_params = conf_loader["parameters"]
constants = conf_loader.get("constants*")
conf_catalog = conf_loader.get("catalog*", "catalog/**")
catalog = DataCatalog.from_config(conf_catalog)

# load configs
SOURCE_DATA_DIR = constants["feature_engineering_data_source_dir"]
DEST_DATA_DIR = constants["feature_engineering_data_destination_dir"]
NUM_OUTLETS = conf_params["feature_engineering"]["endo"]["num_outlets"]


def feature_engineering_pipeline(
    input_df: pd.DataFrame, output_filename: str
) -> Pipeline:
    """Create a feature engineering pipeline for each outlet. Performs endogenous, exogenous and tsfresh feature engineering for relevant tsfresh features..

    Args:
        input_df (pd.DataFrame): Preprocessed dataframe input.
        output_filename (str): The filename for the output data.

    Returns:
        Pipeline: The kedro pipeline with the added node.
    """
    fe_pipeline = pipeline(
        [
            node(
                func=lambda: run_fe_pipeline(input_df),
                inputs=None,
                outputs=[
                    "df_integrated",
                    "added_exog_feature_list",
                    "added_endo_feature_list",
                ],
            ),
            # node(
            #     func=lambda df_integrated, parameters: run_tsfresh_fe_pipeline(
            #         df_integrated, parameters
            #     ),
            #     inputs=["df_integrated", "parameters"],
            #     outputs="fe_df",
            #     name="tsfresh_extract_features",
            # ),
            # node(
            #     func=lambda fe_df, catalog=data_catalog: save_data(
            #         catalog, fe_df, output_filename
            #     ),
            #     inputs=[
            #         "fe_df",
            #     ],
            #     outputs=None,
            #     name="save_fe_data",
            # ),
            node(
                func=lambda df_integrated, catalog=data_catalog: save_data(
                    catalog, df_integrated, output_filename
                ),
                inputs=[
                    "df_integrated",
                ],
                outputs=None,
                name="save_fe_data",
            ),
        ]
    )
    return fe_pipeline


def save_artefact_pipeline() -> Pipeline:
    """pipeline to save list of endogenous and exogenous engineered features as a artefact in json format.

    Returns:
        Pipeline: pipeline to save list of engineered features
    """
    artefact_pipeline = pipeline(
        [
            node(
                func=lambda added_exog_feature_list, added_endo_feature_list: save_artefacts(
                    added_exog_feature_list, added_endo_feature_list
                ),
                inputs=["added_exog_feature_list", "added_endo_feature_list"],
                outputs=None,
            )
        ]
    )
    return artefact_pipeline


def feature_selection_pipeline(
    input_df: pd.DataFrame, relevance_table_list: list
) -> Pipeline:
    """Saves the top relevant tsfresh features in a json file after performing endogenous and exogenous feature engineering.

    Args:
        input_df (pd.DataFrame): Preprocessed dataframe input.
        relevance_table_list (list): list containing relevance tables generated by each outlet.

    Returns:
        Pipeline: Feature selection pipeline
    """
    fs_pipeline = pipeline(
        [
            node(
                func=lambda: run_fe_pipeline(input_df),
                inputs=None,
                outputs=[
                    "df_integrated",
                    "added_exog_feature_list",
                    "added_endo_feature_list",
                ],
            ),
            node(
                func=lambda df_integrated, parameters, relevance_table_list=relevance_table_list: run_tsfresh_feature_selection(
                    relevance_table_list, df_integrated, parameters
                ),
                inputs=["df_integrated", "parameters"],
                outputs="relevance_table_list",
            ),
        ]
    )
    return fs_pipeline


def save_tsfresh_relevant_features_pipeline():
    """Saves top tsfresh relevant features

    Returns:
        Pipeline: pipeline to save tsfresh relevant features
    """
    tsfresh_relevant_features_pipeline = pipeline(
        [
            node(
                func=lambda relevance_table_list, parameters: save_tsfresh_relevant_features(
                    relevance_table_list, parameters
                ),
                inputs=["relevance_table_list", "parameters"],
                outputs=None,
            )
        ]
    )
    return tsfresh_relevant_features_pipeline


def create_feature_engineering_pipeline(**kwargs) -> Pipeline:
    """Performs endogenous, exogenous and tsfresh feature engineering for the preprocessed datasets.

    Main feature engineering pipeline which contains 1 feature engineering pipeline is created for each outlet. A save artefact pipeline is included to save the endogenous and exogenous engineered features.

    Returns:
        Pipeline: Main feature engineering pipeline.
    """
    # Create an empty chain of pipeline
    main_pipeline = Pipeline([])
    # Load the input and output folders
    input_folder, output_folder = get_input_output_folder(
        SOURCE_DATA_DIR, DEST_DATA_DIR
    )
    # Create artefacts subdirectory
    artefacts_dir = os.path.join(output_folder, "artefacts")
    os.makedirs(artefacts_dir, exist_ok=True)

    # Terminate program by returning if directory does not exist
    if not os.path.exists(input_folder):
        log_string = f"{input_folder} not found. Please check config. Exiting...."
        logging.error(log_string)
    else:
        # List contents in the directory
        logging.info(f"Processing contents in: {input_folder}")
        # load partitioned datasets
        partitioned_input = catalog.load("preprocessed_data")
        for partition_id, partition_load_func in sorted(partitioned_input.items()):
            if "merged_" in partition_id:
                partition_id = partition_id.split("_processed")[0]
                partition_data = partition_load_func()
                output_filename = f"{partition_id}_processed_fe"
                output_file_path = output_folder / (output_filename + ".csv")
                add_dataset_to_catalog(data_catalog, output_filename, output_file_path)
                # Add individual pipeline to main Pipeline
                outlet_pipeline = feature_engineering_pipeline(
                    partition_data, output_filename
                )

                # Append the pipeline
                main_pipeline += pipeline(
                    outlet_pipeline,
                    inputs=None,
                    outputs=None,
                    namespace=f"{partition_id}",
                )

        # pipeline to save endo, exo feature engineered list artefact
        artefact_pipeline = save_artefact_pipeline()
        main_pipeline += pipeline(
            artefact_pipeline,
            inputs=None,
            outputs=None,
            namespace=f"{partition_id}",
        )
        return main_pipeline


def create_feature_selection_pipeline(**kwargs) -> Pipeline:
    """Performs endogenous and exogenous feature engineering for a small number of outlets specified in the config file. Then extract all possible features with tsfresh, and save the top relevant tsfresh engineered features.

    Main feature selection pipeline which contains 1 feature selection pipeline for each outlet (over a small number of outlets). Also contains a save tsfresh relevant features pipeline.

    Returns:
        Pipeline: Main feature engineering pipeline.
    """
    # Create an empty chain of pipeline
    main_pipeline = Pipeline([])
    # Load the input and output folders
    input_folder, output_folder = get_input_output_folder(
        SOURCE_DATA_DIR, DEST_DATA_DIR
    )

    # Create artefacts subdirectory
    artefacts_dir = os.path.join(DEST_DATA_DIR, "artefacts")
    os.makedirs(artefacts_dir, exist_ok=True)

    # Terminate program by returning if directory does not exist
    if not os.path.exists(input_folder):
        log_string = f"{input_folder} not found. Please check config. Exiting...."
        logging.error(log_string)

    else:
        # List out contents in the directory
        logging.info(f"Processing contents in: {input_folder}")
        # create empty list to store relevance tables
        relevance_table_list = []
        # load partitioned datasets
        partitioned_input = catalog.load("preprocessed_data")
        partition_list = []
        for partition_id, partition_load_func in sorted(partitioned_input.items()):
            # extract features and save relevant features based on the first N outlets where N = NUM_OUTLETS
            if len(partition_list) == NUM_OUTLETS:
                break
            if "merged_" in partition_id:
                partition_list.append((partition_id, partition_load_func))
        for partition_id, partition_load_func in partition_list:
            partition_id = partition_id.split("_processed")[0]
            partition_data = partition_load_func()
            output_filename = f"{partition_id}_processed_fe"
            output_file_path = output_folder / (output_filename + ".csv")
            add_dataset_to_catalog(data_catalog, output_filename, output_file_path)
            # Add individual pipeline to main Pipeline
            outlet_pipeline = feature_selection_pipeline(
                partition_data, relevance_table_list
            )
            # Append the pipeline
            main_pipeline += pipeline(
                outlet_pipeline,
                inputs=None,
                outputs=None,
                namespace=f"{partition_id}",
            )

        # pipeline to save tsfresh relevant features
        tsfresh_relevant_features_pipeline = save_tsfresh_relevant_features_pipeline()
        main_pipeline += pipeline(
            tsfresh_relevant_features_pipeline,
            inputs=None,
            outputs=None,
            namespace=f"{partition_id}",
        )
    return main_pipeline
